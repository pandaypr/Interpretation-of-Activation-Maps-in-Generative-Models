# Interpretation-of-Activation-Maps-in-Generative-Models
Recent work in the field of Explainable AI and Computer Vision on CNN based architecture has improved the interpretability of Deep Learning models and helped in visualizing the model pre- dictions. Methods like CAM, Grad-CAM and Guided Grad-CAM have proved the practicality of localized visual attention in the classification and categorization applications. However, not much research has been done on generative models. In our work, we implement Grad-CAM technique on VAE and CVAE models trained on CelebA-HQ dataset and calculate neural attention map. The aim of the project is to build generative models capable of generating controllable human faces and build semantic segmentation of human face, and then investigate methodologies to improve the explainability by applying Explainable AI tech- niques like Grad-CAM, and analysing the effect of altering the model architecture, loss functions, latent space size. Furthermore, we investigate the latent space information of models by modifying the latent node variables.
