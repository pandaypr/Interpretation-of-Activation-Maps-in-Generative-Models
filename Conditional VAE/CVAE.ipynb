{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled22.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxganu6vHYyX"
      },
      "source": [
        "Path for Dataset and Pretrained Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gItSsf3GHUbh"
      },
      "source": [
        "!git clone https://github.com/Harvard-IACS/2019-computefest.git\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtIPJMEgHViB"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/2019-computefest/Wednesday/auto_encoder\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPR2tyzIHVsf"
      },
      "source": [
        "import keras\n",
        "from keras import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Reshape, Dense, Lambda, Flatten\n",
        "from keras import backend as K\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import glob\n",
        "import skimage\n",
        "import skimage.transform\n",
        "import skimage.io\n",
        "import PIL\n",
        "import numpy as np\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "import imageio\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LjKj83qHmI7"
      },
      "source": [
        "VAE encoder-decoder architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvoQQ8hRHVu1"
      },
      "source": [
        "def define_encoder_block(x, num_filters):  \n",
        "    \"\"\"\n",
        "    Todo: Define two sequential 2D convolutional layers (Conv2D) with the following properties:\n",
        "          - num_filters many filters\n",
        "          - kernel_size 3\n",
        "          - activation \"relu\"\n",
        "          - padding \"same\"\n",
        "          - kernel_initializer \"he_normal\"\n",
        "          Also define a 2D max pooling layer (MaxPooling2D) (you can keep default arguments).\n",
        "    \"\"\"\n",
        "    x = Conv2D(num_filters, 3, activation='relu', padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = Conv2D(num_filters, 3, activation='relu', padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a2VekJyHVxN"
      },
      "source": [
        "def define_decoder_block(x, num_filters):\n",
        "    \"\"\"\n",
        "    Todo: Define one 2D upsampling layer (UpSampling2D) (you can keep default arguments).\n",
        "          Also, define two sequential 2D convolutional layers (Conv2D) with the following properties:\n",
        "          - num_filters many filters\n",
        "          - kernel_size 3\n",
        "          - activation \"relu\"\n",
        "          - padding \"same\"\n",
        "          - kernel_initializer \"he_normal\"\n",
        "    \"\"\"\n",
        "    x = UpSampling2D()(x)\n",
        "    x = Conv2D(num_filters, 3, activation='relu', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
        "    x = Conv2D(num_filters, 3, activation='relu', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLu04rbmHWBY"
      },
      "source": [
        "def define_net(variational, height, width, batch_size, latent_dim, conditioning_dim=0,\n",
        "               start_filters=8):\n",
        "    \"\"\"Defines a (variational) encoder-decoder architecture.\n",
        "    \n",
        "    Args:\n",
        "        variational: Whether a variational autoencoder should be defined.\n",
        "        height: The height of the image input and output.\n",
        "        width: The width of the image input and output.\n",
        "        batch_size: The batchsize that is used during training. Must also be used for inference on the encoder side.\n",
        "        latent_dim: The dimension of the latent space.\n",
        "        conditioning_dim: The dimension of the space of variables to condition on. Can be zero for an unconditional VAE.\n",
        "        start_filters: The number of filters to start from. Multiples of this value are used across the network. Can be used\n",
        "            to change model capacity.\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of keras models for full VAE, encoder part and decoder part only.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Prepare the inputs.\n",
        "    inputs = Input((height, width, 3))\n",
        "    if conditioning_dim > 0:\n",
        "        # Define conditional VAE. Note that this is usually not the preferred way\n",
        "        # of incorporating the conditioning information in the encoder.\n",
        "        condition = Input([conditioning_dim])\n",
        "        condition_up = Dense(height * width)(condition)\n",
        "        condition_up = Reshape([height, width, 1])(condition_up)\n",
        "        inputs_new = Concatenate(axis=3)([inputs, condition_up])\n",
        "    else:\n",
        "        inputs_new = inputs\n",
        "    \n",
        "    # Define the encoder.\n",
        "    eblock1 = define_encoder_block(inputs_new, start_filters)\n",
        "    eblock2 = define_encoder_block(eblock1, start_filters*2)\n",
        "    eblock3 = define_encoder_block(eblock2, start_filters*4)\n",
        "    eblock4 = define_encoder_block(eblock3, start_filters*8)\n",
        "    _, *shape_spatial = eblock4.get_shape().as_list()\n",
        "    eblock4_flat = Flatten()(eblock4)\n",
        "    \n",
        "    if not variational:\n",
        "        z = Dense(latent_dim)(eblock4_flat)\n",
        "    else:\n",
        "        # Perform the sampling.\n",
        "        def sampling(args):\n",
        "            \"\"\"Samples latent variable from a normal distribution using the given parameters.\"\"\"\n",
        "            z_mean, z_log_sigma = args\n",
        "            epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1.)\n",
        "            return z_mean + K.exp(z_log_sigma) * epsilon\n",
        "        \n",
        "        z_mean = Dense(latent_dim)(eblock4_flat)\n",
        "        z_log_sigma = Dense(latent_dim)(eblock4_flat)\n",
        "        z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
        "    \n",
        "    if conditioning_dim > 0:\n",
        "        z_ext = Concatenate()([z, condition])\n",
        "\n",
        "    # Define the decoder.\n",
        "    inputs_embedding = Input([latent_dim + conditioning_dim])\n",
        "    embedding = Dense(np.prod(shape_spatial), activation='relu')(inputs_embedding)\n",
        "    embedding = Reshape(eblock4.shape.as_list()[1:])(embedding)\n",
        "    \n",
        "    dblock1 = define_decoder_block(embedding, start_filters*8)\n",
        "    dblock2 = define_decoder_block(dblock1, start_filters*4)\n",
        "    dblock3 = define_decoder_block(dblock2, start_filters*2)\n",
        "    dblock4 = define_decoder_block(dblock3, start_filters)\n",
        "    output = Conv2D(3, 1, activation = 'tanh')(dblock4)\n",
        "    \n",
        "    # Define the models.\n",
        "    decoder = Model(inputs = inputs_embedding, outputs = output)\n",
        "    if conditioning_dim > 0:\n",
        "        encoder_with_sampling = Model(inputs = [inputs, condition], outputs = z)\n",
        "        encoder_with_sampling_ext = Model(inputs = [inputs, condition], outputs = z_ext)\n",
        "        vae_out = decoder(encoder_with_sampling_ext([inputs, condition]))\n",
        "        vae = Model(inputs = [inputs, condition], outputs = vae_out)\n",
        "    else:\n",
        "        encoder_with_sampling = Model(inputs = inputs, outputs = z)\n",
        "        vae_out = decoder(encoder_with_sampling(inputs))\n",
        "        vae = Model(inputs = inputs, outputs = vae_out)\n",
        "    \n",
        "    # Define the VAE loss.\n",
        "    def vae_loss(x, x_decoded_mean):\n",
        "        \"\"\"Defines the VAE loss functions as a combination of MSE and KL-divergence loss.\"\"\"\n",
        "        mse_loss = K.mean(keras.losses.mse(x, x_decoded_mean), axis=(1,2)) * height * width\n",
        "        kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
        "        return mse_loss + kl_loss\n",
        "    \n",
        "    if variational:\n",
        "        vae.compile(loss=vae_loss, optimizer='adam')\n",
        "    else:\n",
        "        vae.compile(loss='mse', optimizer='adam')    \n",
        "        \n",
        "    print('done,', vae.count_params(), 'parameters.')\n",
        "    return vae, encoder_with_sampling, decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uve2KfYPHWDq"
      },
      "source": [
        "def encode_image(img, conditioning, encoder, height, width, batch_size):\n",
        "    '''Encodes an image that is given in RGB-channel order with value range of [0, 255].\n",
        "    \n",
        "    Args:\n",
        "        img: The image input. If shapes differ from (height, width), it will be resized.\n",
        "        conditoning: The set of values to condition on, if any. Can be None.\n",
        "        encoder: The keras encoder model to use.\n",
        "        height: The target image height.\n",
        "        width: The target image width.\n",
        "        batch_size: The batchsize that the encoder expects.\n",
        "        \n",
        "    Returns:\n",
        "        The latent representation of the input image.\n",
        "    '''\n",
        "    if img.shape[0] != height or img.shape[1] != width:\n",
        "        img = skimage.transform.resize(img, (height, width))\n",
        "    img_single = np.expand_dims(img, axis=0)\n",
        "    img_single = img_single.astype(np.float32)\n",
        "    img_single = np.repeat(img_single, batch_size, axis=0)\n",
        "    if conditioning is None:\n",
        "        z = encoder.predict(img_single)\n",
        "    else:\n",
        "        z = encoder.predict([img_single, np.repeat(np.expand_dims(conditioning, axis=0), batch_size, axis=0)])\n",
        "    return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNZRPZ_mHWGD"
      },
      "source": [
        "def decode_embedding(z, conditioning, decoder):\n",
        "    '''Decodes the given representation into an image.\n",
        "    \n",
        "    Args:\n",
        "        z: The latent representation.\n",
        "        conditioning: The set of values to condition on, if any. Can be None.\n",
        "        decoder: The keras decoder model to use.\n",
        "    '''\n",
        "    if z.ndim < 2:\n",
        "        z = np.expand_dims(z, axis=0) # Single-batch\n",
        "    if conditioning is not None:\n",
        "        z = np.concatenate((z, np.repeat(np.expand_dims(conditioning, axis=0), z.shape[0], axis=0)), axis=1)\n",
        "    return decoder.predict(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFxWKTTZIFvg"
      },
      "source": [
        "Load Weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhD-03J3IDUg"
      },
      "source": [
        "def load_weights(folder):\n",
        "    vae.load_weights(folder + '/vae.w')\n",
        "    encoder.load_weights(folder + '/encoder.w')\n",
        "    decoder.load_weights(folder + '/decoder.w')\n",
        "    \n",
        "def save_weights(folder):\n",
        "    if not os.path.isdir(folder):\n",
        "        os.mkdir(folder)\n",
        "    vae.save_weights(folder + '/vae.w')\n",
        "    encoder.save_weights(folder + '/encoder.w')\n",
        "    decoder.save_weights(folder + '/decoder.w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AakZeYmwIDXn"
      },
      "source": [
        "VARIATIONAL = True\n",
        "HEIGHT = 128\n",
        "WIDTH = 128\n",
        "BATCH_SIZE = 16\n",
        "LATENT_DIM = 16\n",
        "START_FILTERS = 32\n",
        "CONDITIONING = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4o9e8hNIDbk"
      },
      "source": [
        "import tensorflow\n",
        "class CustomDataGenerator(tensorflow.keras.utils.Sequence):\n",
        "    def __init__(self, files, batch_size, target_height, target_width, conditioning_dim=0, conditioning_data=None):\n",
        "        '''\n",
        "        Intializes the custom generator.\n",
        "        \n",
        "        Args:\n",
        "            files: The list of paths to images that should be fed to the network.\n",
        "            batch_size: The batchsize to use.\n",
        "            target_height: The target image height. If different, the images will be resized.\n",
        "            target_width: The target image width. If different, the images will be resized.\n",
        "            conditioning_dim: The dimension of the conditional variable space. Can be 0.\n",
        "            conditioning_data: Optional dictionary that maps from the filename to the data to be\n",
        "                conditioned on. Data must be numeric. Can be None. Otherwise, len must be equal to\n",
        "                conditioning_dim.\n",
        "        '''\n",
        "        self.files = files\n",
        "        self.batch_size = batch_size\n",
        "        self.target_height = target_height\n",
        "        self.target_width = target_width\n",
        "        self.conditioning_dim = conditioning_dim\n",
        "        self.conditioning_data = conditioning_data\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        '''Shuffle list of files after each epoch.'''\n",
        "        np.random.shuffle(self.files)\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        cur_files = self.files[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(cur_files)\n",
        "        return X, y\n",
        "    \n",
        "    def __data_generation(self, cur_files):\n",
        "        X = np.empty(shape=(self.batch_size, self.target_height, self.target_width, 3))\n",
        "        Y = np.empty(shape=(self.batch_size, self.target_height, self.target_width, 3))\n",
        "        if self.conditioning_data != None:\n",
        "            C = np.empty(shape=(self.batch_size, self.conditioning_dim))\n",
        "        \n",
        "        for i, file in enumerate(cur_files):\n",
        "            img = skimage.io.imread(file)\n",
        "            if img.shape[0] != self.target_height or img.shape[1] != self.target_width:\n",
        "                img = skimage.transform.resize(img, (self.target_height, self.target_width)) # Resize.\n",
        "            img = img.astype(np.float32) / 255.\n",
        "            X[i] = img\n",
        "            Y[i] = img\n",
        "            if self.conditioning_data != None:\n",
        "                C[i] = self.conditioning_data[os.path.basename(file)]\n",
        "                \n",
        "        if self.conditioning_data != None:\n",
        "            return [X, C], Y\n",
        "        else:\n",
        "            return X, Y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.files) / self.batch_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc3RP19eIt5l"
      },
      "source": [
        "Conditions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAO_sre9Irng"
      },
      "source": [
        "# Find image files.\n",
        "files = glob.glob('celeba/img_align_celeba/*.jpg')\n",
        "print(len(files), 'images found.')\n",
        "\n",
        "df = utils.load_celeba('celeba/list_attr_celeba.txt')\n",
        "columns = df.columns\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WGJActPIrx5"
      },
      "source": [
        "dd = {}\n",
        "selected_conditionals = list(columns[1:])\n",
        "for i, row in df.iterrows():\n",
        "    dd[row['Filename']] = [int(row[c]) for c in selected_conditionals]\n",
        "\n",
        "gen = CustomDataGenerator(files=files, \n",
        "                          batch_size=BATCH_SIZE, \n",
        "                          target_height=HEIGHT, \n",
        "                          target_width=WIDTH, \n",
        "                          conditioning_dim=len(selected_conditionals),\n",
        "                          conditioning_data=dd if CONDITIONING else None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALTyPGl5I1Oz"
      },
      "source": [
        "Define CVAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRXdtvqBIr0X"
      },
      "source": [
        "vae, encoder, decoder = define_net(variational=VARIATIONAL,\n",
        "                                   height=HEIGHT, \n",
        "                                   width=WIDTH, \n",
        "                                   batch_size=BATCH_SIZE, \n",
        "                                   latent_dim=LATENT_DIM,\n",
        "                                   conditioning_dim=len(selected_conditionals) if CONDITIONING else 0, \n",
        "                                   start_filters=START_FILTERS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X5KsvulIr5E"
      },
      "source": [
        "load_weights(folder='models/celeba_vae')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIcT-D0uI6ib"
      },
      "source": [
        "Let's look at some examples. First, we will select a random image from the CelebA dataset, and read the related annotation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0_XYDFkIr8M"
      },
      "source": [
        "rnd_file = np.random.choice(files)\n",
        "file_id = os.path.basename(rnd_file)\n",
        "init_meta = dd[file_id]\n",
        "img = skimage.io.imread(rnd_file)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaUxoEbrJDPr"
      },
      "source": [
        "Encode Image to Latent Dimension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsn2hgIdI97c"
      },
      "source": [
        "z = encode_image(img.astype(np.float32) / 255., np.array(init_meta), encoder, HEIGHT, WIDTH, BATCH_SIZE)\n",
        "print('latent sample:\\n', z[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrNgQJYqJJLk"
      },
      "source": [
        "Decode Latent information to Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1mpwQPSI9-O"
      },
      "source": [
        "ret = decode_embedding(z, init_meta, decoder)\n",
        "plt.imshow(ret[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krK8NL5NJuhi"
      },
      "source": [
        "HeatMAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IffFBP5vI-Al"
      },
      "source": [
        "example_batch = next(data_flow)\n",
        "example_batch = example_batch[0]\n",
        "example_images = example_batch[:10]\n",
        "img_array = example_images[0]\n",
        "matplotlib.pyplot.imshow(img_array)\n",
        "img_array = tf.keras.preprocessing.image.img_to_array(img_array)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "res = vae_model.predict(img_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFc8QYSvJjvv"
      },
      "source": [
        "import matplotlib\n",
        "res = res.reshape((128,128,3))\n",
        "matplotlib.pyplot.imshow(res[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbFue7QEJjyO"
      },
      "source": [
        "last_conv_layer_name = \"encoder_conv_3\"\n",
        "encoder_out = \"encoder_output\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb-OwIl-Jj0t"
      },
      "source": [
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, encoder_out, pred_index=None):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.get_layer(encoder_out).output]\n",
        "    )\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "    \n",
        "    grads = tape.gradient(preds, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    # then sum all the channels to obtain the heatmap class activation\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = tf.matmul(last_conv_layer_output, pooled_grads[..., tf.newaxis])\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap, grads, pooled_grads, preds, last_conv_layer_output\n",
        "In [ ]:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp2mt-tRJpJz"
      },
      "source": [
        "heatmap, grads, pooled_grads, preds, last_conv_layer_output = make_gradcam_heatmap(img_array, vae_model, last_conv_layer_name, encoder_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMkgR7MDJpZ6"
      },
      "source": [
        "print(preds)\n",
        "print(last_conv_layer_output)\n",
        "print(grads)\n",
        "print(pooled_grads)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x185E9FqJtV4"
      },
      "source": [
        "def save_and_display_gradcam(img, heatmap,cam_path=\"superimposed_img.jpg\", alpha=0.4):\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    jet = cm.get_cmap(\"jet\")\n",
        "    jet_colors = jet(np.arange(256))[:, :3]\n",
        "    jet_heatmap = jet_colors[heatmap]\n",
        "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
        "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
        "    superimposed_img = cv2.addWeighted(jet_heatmap, 0.005, img, 0.995, 0)\n",
        "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
        "    superimposed_img.save(cam_path)\n",
        "\n",
        "save_and_display_gradcam(img_array.squeeze(), heatmap)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
